{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e0e934-3550-4fa1-b341-6d27d527a357",
   "metadata": {},
   "source": [
    "In this example, we use cycleGAN to convert horse images into zebra images (and vice versa).\n",
    "\n",
    "cycleGAN paper: https://arxiv.org/abs/1703.10593\n",
    "\n",
    "Dataset from: http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/horse2zebra.zip\n",
    "\n",
    "cycleGAN Architecture:\n",
    "\n",
    "<img src=\"https://blog.jaysinha.me/content/images/size/w2000/2023/03/cyclegan.png\" alt=\"cycleGAN Architecture\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d14a8-c7e6-4d8e-98e6-9aab125bfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, Concatenate, Dropout, BatchNormalization, Activation\n",
    "from keras.initializers import RandomNormal\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.models import Model\n",
    "try:\n",
    "    from keras.optimizers.legacy import Adam\n",
    "except ImportError:\n",
    "    from keras.optimizers import Adam\n",
    "from keras.utils import plot_model, set_random_seed, image_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7f412-95f2-4235-b2d5-e9a9c1233e25",
   "metadata": {},
   "source": [
    "Some user inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ae724-fa62-47b0-adaf-c30451fd1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../datasets/horse2zebra/\"\n",
    "set_random_seed(1000) # Sets all random seeds (Python, NumPy, and backend framework, e.g. TF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81083c-ad2e-4ef5-9cfd-3093ef91df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, size=(256,256)):\n",
    "    imgs = []\n",
    "    for filename in listdir(path):\n",
    "        im = image_utils.load_img(path + filename, target_size=size)\n",
    "        im = image_utils.img_to_array(im)\n",
    "        im = (im - 127.5) / 127.5 # normalize pixel values to [-1,+1]\n",
    "        imgs.append(im)\n",
    "    return np.array(imgs)\n",
    "\n",
    "horse_images = load_images(data_path + 'trainA/')[:300]\n",
    "zebra_images = load_images(data_path + 'trainB/')[:300]\n",
    "training_dataset = [horse_images, zebra_images]  # define training dataset as [domainA, domainB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed4060-c979-4f8b-89f6-c1080f439bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildD(input_shape): # the function to build the discriminator network\n",
    "    init = RandomNormal(stddev=0.02, seed=1000)\n",
    "    input_img = Input(shape=input_shape)\n",
    "    \n",
    "    out = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(input_img)\n",
    "    out = LeakyReLU(alpha=0.2)(out)\n",
    "\n",
    "    out = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(out)\n",
    "    out = InstanceNormalization(axis=-1)(out)\n",
    "    out = LeakyReLU(alpha=0.2)(out)\n",
    "\n",
    "    out = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(out)\n",
    "    out = InstanceNormalization(axis=-1)(out)\n",
    "    out = LeakyReLU(alpha=0.2)(out)\n",
    "    \n",
    "    # Not in the original paper. Comment this block if you want.\n",
    "    out = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(out)\n",
    "    out = InstanceNormalization(axis=-1)(out)\n",
    "    out = LeakyReLU(alpha=0.2)(out)\n",
    "\n",
    "    out = Conv2D(512, (4,4), strides=(1,1), padding='same', kernel_initializer=init)(out)\n",
    "    out = InstanceNormalization(axis=-1)(out)\n",
    "    out = LeakyReLU(alpha=0.2)(out)\n",
    "\n",
    "    out = Conv2D(1, (4,4), strides=(1,1), padding='same', activation='sigmoid', kernel_initializer=init)(out)\n",
    "    model = Model(input_img, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), loss_weights=[0.5])\n",
    "    return model\n",
    "    \n",
    "horseD = buildD(horse_images.shape[1:])\n",
    "zebraD = buildD(zebra_images.shape[1:])\n",
    "try:\n",
    "    horseD.load_weights('cycleGAN_horseD_weights.h5')\n",
    "    zebraD.load_weights('cycleGAN_zebraD_weights.h5')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "horseD.summary(expand_nested=True)\n",
    "plot_model(horseD, show_shapes=True, expand_nested=True, show_layer_activations=True, show_layer_names=False, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c32ed-18a2-43e8-9962-e1c790dbd79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildG(input_shape): # the function to build the generator network\n",
    "    init = RandomNormal(stddev=0.02, seed=1000)\n",
    "    \n",
    "    def resnet_block(n_filters, input_layer):\n",
    "    \t# first convolutional layer\n",
    "    \tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "    \tg = InstanceNormalization(axis=-1)(g)\n",
    "    \tg = Activation('relu')(g)\n",
    "    \t# second convolutional layer\n",
    "    \tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "    \tg = InstanceNormalization(axis=-1)(g)\n",
    "    \t# concatenate merge channel-wise with input layer\n",
    "    \tg = Concatenate()([g, input_layer])\n",
    "    \treturn g\n",
    "\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "\t# c7s1-64\n",
    "    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(input_img)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d128\n",
    "    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "\t# d256\n",
    "    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "\t# R256\n",
    "    for _ in range(9): # addind 9 ResNet blocks\n",
    "        g = resnet_block(256, g)\n",
    "\t# u128\n",
    "    g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "\t# u64\n",
    "    g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "\t# c7s1-3\n",
    "    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    out_image = Activation('tanh')(g)\n",
    "\n",
    "    return Model(input_img, out_image)\n",
    "    \n",
    "horseG = buildG(horse_images.shape[1:])\n",
    "zebraG = buildG(zebra_images.shape[1:])\n",
    "try:\n",
    "    horseG.load_weights('cycleGAN_horseG_weights.h5')\n",
    "    zebraG.load_weights('cycleGAN_zebraG_weights.h5')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "horseG.summary(expand_nested=True)\n",
    "plot_model(horseG, show_shapes=True, expand_nested=True, show_layer_activations=True, show_layer_names=False, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2f066-70f6-42c6-9dc5-9a6133417209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function combines the generator and the discriminator in order to build the cycleGAN network\n",
    "def buidGAN(g_model_1, d_model_1, g_model_2, input_shape):\n",
    "    g_model_1.trainable = True\n",
    "    d_model_1.trainable = False\n",
    "    g_model_2.trainable = False\n",
    "\n",
    "    input_img1 = Input(shape=input_shape)\n",
    "    # adversarial loss\n",
    "    g1_out = g_model_1(input_img1)\n",
    "    d1_out = d_model_1(g1_out)\n",
    "    # identity loss\n",
    "    input_img2 = Input(shape=input_shape)\n",
    "    g1_out_identity = g_model_1(input_img2)\n",
    "    # cycle loss - forward\n",
    "    g2_out_forward = g_model_2(g1_out)\n",
    "    # cycle loss - backward\n",
    "    g1_out_backward = g_model_1(g_model_2(input_img2))\n",
    "\n",
    "    model = Model([input_img1, input_img2], [d1_out, g1_out_identity, g2_out_forward, g1_out_backward])\n",
    "    model.compile(loss=['binary_crossentropy', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=Adam(0.0002, 0.5))\n",
    "    return model\n",
    "\n",
    "horseGAN = buidGAN(horseG, horseD, zebraG, horse_images.shape[1:])\n",
    "zebraGAN = buidGAN(zebraG, zebraD, horseG, zebra_images.shape[1:])\n",
    "horseGAN.summary(expand_nested=True)\n",
    "plot_model(horseGAN, show_shapes=True, expand_nested=True, show_layer_activations=True, show_layer_names=False, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52518de5-1129-4215-82f4-bd4a19339f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a batch of random samples\n",
    "def generate_real_samples(data, n_samples, patch_shape):\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, data.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = data[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate a batch of fake images\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(samples, verbose=0)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate samples and plot them\n",
    "def plot_sample_images(g_model_1, g_model_2, dataset):\n",
    "    # select a sample of images from domain1 and domain2\n",
    "    input_images1, _ = generate_real_samples(dataset[0], 1, 1)\n",
    "    input_images2, _ = generate_real_samples(dataset[1], 1, 1)\n",
    "    # domain1 -> domain2 test: calculating generator1's output and passing it to generator2 (we would expect to see the primary input images as result)\n",
    "    g12_1, _ = generate_fake_samples(g_model_1, input_images1, 1)\n",
    "    g21_1, _ = generate_fake_samples(g_model_2, g12_1, 1)\n",
    "    # domain2 -> domain1 test: calculating generator2's output and passing it to generator1 (we would expect to see the primary input images as result)\n",
    "    g21_2, _ = generate_fake_samples(g_model_2, input_images2, 1)\n",
    "    g12_2, _ = generate_fake_samples(g_model_1, g21_2, 1)\n",
    "    # identity test: passing an image from domain2 to generator1 should output the same input image. the same applies for domain1 and generator2.\n",
    "    g11, _ = generate_fake_samples(g_model_1, input_images2, 1)\n",
    "    g22, _ = generate_fake_samples(g_model_2, input_images1, 1)\n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    input_images1 = (input_images1 + 1) / 2.0\n",
    "    input_images2 = (input_images2 + 1) / 2.0\n",
    "    g12_1 = (g12_1 + 1) / 2.0\n",
    "    g21_1 = (g21_1 + 1) / 2.0\n",
    "    g12_2 = (g12_2 + 1) / 2.0\n",
    "    g12_2 = (g12_2 + 1) / 2.0\n",
    "    g11 =   (g11 + 1) / 2.0\n",
    "    g22 =   (g22 + 1) / 2.0\n",
    "\n",
    "    # plot real source images\n",
    "    fig, axs = plt.subplots(4, 3, figsize=(10,10))\n",
    "    axs[0,0].imshow(input_images1[0])\n",
    "    axs[0,1].imshow(g12_1[0])\n",
    "    axs[0,2].imshow(g21_1[0])\n",
    "    axs[0,0].set_title(\"domain1 ---->\", size=10)\n",
    "    axs[0,1].set_title(\"generator1 ---->\", size=10)\n",
    "    axs[0,2].set_title(\"generator2 (reconstructed)\", size=10)\n",
    "    \n",
    "    axs[1,0].imshow(input_images2[0])\n",
    "    axs[1,1].imshow(g21_2[0])\n",
    "    axs[1,2].imshow(g12_2[0])\n",
    "    axs[1,0].set_title(\"domain2 ---->\", size=10)\n",
    "    axs[1,1].set_title(\"generator2 ---->\", size=10)\n",
    "    axs[1,2].set_title(\"generator1 (reconstructed)\", size=10)\n",
    "    \n",
    "    axs[2,0].imshow(input_images2[0])\n",
    "    axs[2,1].imshow(g11[0])\n",
    "    axs[2,0].set_title(\"domain2 ---->\", size=10)\n",
    "    axs[2,1].set_title(\"generator1 (identity test)\", size=10)\n",
    "    \n",
    "    axs[3,0].imshow(input_images1[0])\n",
    "    axs[3,1].imshow(g22[0])\n",
    "    axs[3,0].set_title(\"domain1 ---->\", size=10)\n",
    "    axs[3,1].set_title(\"generator2 (identity test)\", size=10)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            axs[i,j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cef9f-63d0-40f8-8686-86a61b696a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pix2pix model\n",
    "def train(g_model_1, d_model_1, g_model_2, d_model_2, gan_model_1, gan_model_2, dataset, n_epochs=100, n_batch=1, plot_interval=10):    \n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch = d_model_1.output_shape[1]\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(dataset[0]) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # manually enumerate epochs\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        # select a batch of real samples from domain1 and domain2\n",
    "        data1, y_real_1 = generate_real_samples(dataset[0], n_batch, n_patch)\n",
    "        data2, y_real_2 = generate_real_samples(dataset[1], n_batch, n_patch)\n",
    "        # generate a batch of fake samples\n",
    "        g1, y_fake_1 = generate_fake_samples(g_model_1, data1, n_patch)\n",
    "        g2, y_fake_2 = generate_fake_samples(g_model_2, data2, n_patch)\n",
    "        # update discriminator for real samples\n",
    "        d_model_1.train_on_batch(data2, y_real_2)\n",
    "        d_model_2.train_on_batch(data1, y_real_1)\n",
    "        # update discriminator for generated samples\n",
    "        d_model_1.train_on_batch(g1, y_fake_1)\n",
    "        d_model_2.train_on_batch(g2, y_fake_2)\n",
    "        # update the generator\n",
    "        gan_model_1.train_on_batch([data1, data2], [y_real_1, data2, data1, data2])\n",
    "        gan_model_2.train_on_batch([data2, data1], [y_real_2, data1, data2, data1])\n",
    "        if i % (bat_per_epo*plot_interval-1) == 0:\n",
    "            plot_sample_images(g_model_1, g_model_2, dataset)\n",
    "            # save the model weights\n",
    "            d_model_1.save_weights('cycleGAN_horseD_weights.h5')\n",
    "            g_model_1.save_weights('cycleGAN_horseG_weights.h5')\n",
    "            d_model_2.save_weights('cycleGAN_zebraD_weights.h5')\n",
    "            g_model_2.save_weights('cycleGAN_zebraG_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a9ad3-3395-48bb-aa45-bcfdb9a2cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(horseG, horseD, zebraG, zebraD, horseGAN, zebraGAN, training_dataset, n_epochs=10, n_batch=1, plot_interval=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
