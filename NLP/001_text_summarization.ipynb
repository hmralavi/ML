{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf52b7da-5067-4557-b172-a6a553f37063",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224a4a70-db97-48a2-b5ad-f733119d9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Reshape, TimeDistributed, Attention, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from emoji import replace_emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e98435-7563-4417-95c9-ecb10f132b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary for contracition mapping \n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a52c5d8-8bec-4f7a-9b8e-86222b4c2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(txt, sum):\n",
    "    def f(txt):\n",
    "        newtxt = txt.decode('utf-8')\n",
    "        newtxt = replace_emoji(newtxt,'')\n",
    "        newtxt = newtxt.lower()\n",
    "        newtxt = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newtxt.split(\" \")])\n",
    "        return newtxt\n",
    "    txt = f(txt)\n",
    "    sum = \"soseq \" + f(sum) + \" eoseq\"\n",
    "    return txt, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3712f94-ffbc-4e29-b5e1-1d055bb609e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = tfds.load(name='samsum', split=['train[:10%]','test'], as_supervised=True)\n",
    "ds_train = np.array([text_preprocess(*d) for d in ds_train.as_numpy_iterator()]) # ndarray shape:(14732, 2)\n",
    "ds_test = np.array([text_preprocess(*d) for d in ds_test.as_numpy_iterator()]) # ndarray shape:(819, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a83294-6b5a-41ba-884b-a75fc92ecfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(txt_to_fit, min_token_count=1):\n",
    "    filt = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r'\n",
    "    tokenizer = Tokenizer(filters=filt)\n",
    "    tokenizer.fit_on_texts(txt_to_fit)\n",
    "    count = 0 # Number of rare words \n",
    "    total_count = 0 # count of every unique word in texts \n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        # update counts and frequencies\n",
    "        total_count += 1\n",
    "        # if the value of word was lower than our threshold, count!\n",
    "        if value < min_token_count:\n",
    "            count += 1\n",
    "    # get the top most common words number \n",
    "    common_words = total_count - count\n",
    "    tokenizer = Tokenizer(filters=filt, num_words=common_words)\n",
    "    tokenizer.fit_on_texts(txt_to_fit)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3662a54-4076-44aa-890e-d538bd2aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target texts in the train dataset\n",
    "tokenizer_input = create_tokenizer(ds_train[:,0], 300)\n",
    "input_sequences = tokenizer_input.texts_to_sequences(ds_train[:,0]) # list of length 14732\n",
    "\n",
    "tokenizer_target = create_tokenizer(ds_train[:,1], 20)\n",
    "target_sequences = tokenizer_target.texts_to_sequences(ds_train[:,1]) # list of length 14732\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = pad_sequences(input_sequences, padding='post', truncating='post') # ndarray shape:(14732, 813)\n",
    "target_sequences = pad_sequences(target_sequences, padding='post', truncating='post') # ndarray shape:(14732, 64)\n",
    "\n",
    "# Prepare target data\n",
    "target_sequences_one_hot = tf.keras.utils.to_categorical(target_sequences, \n",
    "                                                         num_classes=tokenizer_target.num_words+1) # ndarray shape (14732, 64, 17599)\n",
    "\n",
    "# \n",
    "x_train = [input_sequences, target_sequences[:,:-1]]\n",
    "y_train = target_sequences_one_hot[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fa39ef-b3cf-4dd3-90dd-307954213001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence nvocabs: 72\n",
      "input sequence shape: (1473, 266)\n",
      "sample: ['i just to you know that i a really time with you yeah i really as well if you are up for it i would really like to see you i am but i have a really up yeah no i but if you to go just me know yeah of for sure have a']\n",
      "\n",
      "\n",
      "target sequence nvocabs: 192\n",
      "target sequence shape: (1473, 45)\n",
      "target sequence shape (one-hotted): (1473, 45, 193)\n",
      "sample: ['soseq and tonight would like to meet but is eoseq']\n"
     ]
    }
   ],
   "source": [
    "print(\"input sequence nvocabs:\", tokenizer_input.num_words)\n",
    "print(\"input sequence shape:\", input_sequences.shape)\n",
    "print(\"sample:\", tokenizer_input.sequences_to_texts([input_sequences[0]]))\n",
    "print('\\n')\n",
    "print(\"target sequence nvocabs:\", tokenizer_target.num_words)\n",
    "print(\"target sequence shape:\", target_sequences.shape)\n",
    "print(\"target sequence shape (one-hotted):\", target_sequences_one_hot.shape)\n",
    "print(\"sample:\", tokenizer_target.sequences_to_texts([target_sequences[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28d4fe5-950a-4dea-8a30-ca852accc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(ntokens_input, ntokens_target):  \n",
    "    LATENT_DIM = 250\n",
    "    EMBED_DIM = 90\n",
    "\n",
    "    #----------- Encoder ---------------\n",
    "    encoder_input = Input(shape=(None, ), name=\"encoder_input\")\n",
    "    # Embedding \n",
    "    encoder_embed = Embedding(ntokens_input, EMBED_DIM, trainable=True, mask_zero=False, name=\"encoder_embeding\")(encoder_input)\n",
    "    # Encoder LSTM Block 1\n",
    "    encoder_lstm1 = LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4, name=\"enc_lstm_1\")\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_embed)\n",
    "    # Encoder LSTM Block 2\n",
    "    encoder_lstm2 = LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4, name=\"enc_lstm_2\")\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "    # Encoder LSTM Block 3\n",
    "    encoder_lstm3 = LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4, name=\"enc_lstm_3\")\n",
    "    encoder_output3, state_h3, state_c3 = encoder_lstm3(encoder_output2)\n",
    "\n",
    "    #------------ Decoder --------------\n",
    "    # define the input \n",
    "    decoder_input = Input(shape=(None, ), name=\"decoder_input\")\n",
    "    # Embedding for Decoder \n",
    "    decoder_embed_layer = Embedding(ntokens_target, EMBED_DIM, trainable=True)\n",
    "    decoder_embed = decoder_embed_layer(decoder_input)\n",
    "    # Decoder LSTM\n",
    "    decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4, name=\"decoder_lstm\")\n",
    "    decoder_lstm_output, decoder_h1, decoder_c1 = decoder_lstm(decoder_embed, initial_state=[state_h3, state_c3])\n",
    "    # Attention Layer use this:\n",
    "    # from attention import AttentionLayer\n",
    "    # attention_layer = AttentionLayer(name=\"attention_layer\")\n",
    "    # attention_output, attention_state = attention_layer([encoder_output3, decoder_lstm_output])\n",
    "    # Attention Layer OR use this:\n",
    "    attention_layer = Attention(name=\"attention_layer\")\n",
    "    attention_output = attention_layer([decoder_lstm_output, encoder_output3])\n",
    "    # concatinate decoder and attention otuputs \n",
    "    decoder_concat = Concatenate(axis=-1, name=\"decoder_concat\")([decoder_lstm_output, attention_output])\n",
    "    # Time Distributed Dense \n",
    "    decoder_dense = TimeDistributed(Dense(ntokens_target, activation=\"softmax\"))\n",
    "    decoder_output = decoder_dense(decoder_concat)\n",
    "    \n",
    "    # define single model for encoder\n",
    "    def standalone_encoder():\n",
    "        return tf.keras.models.Model(encoder_input, outputs=[encoder_output3, state_h3, state_c3])\n",
    "\n",
    "    # define single model for decoder\n",
    "    def standalone_decoder():\n",
    "        # define inputs \n",
    "        decoder_input_state_h = Input(shape=(LATENT_DIM, ))\n",
    "        decoder_input_state_c = Input(shape=(LATENT_DIM, ))\n",
    "        decoder_input_state_hidden = Input(shape=(None, LATENT_DIM))\n",
    "        \n",
    "        # Embedding of the decoder \n",
    "        decoder_embedding_pred = decoder_embed_layer(decoder_input)\n",
    "        # Decoder LSTM (For making prediction, the state of the decoder should be set to the state of its previos time step)\n",
    "        decoder_output_p, state_h_p, state_c_p = decoder_lstm(decoder_embedding_pred, initial_state=[decoder_input_state_h, decoder_input_state_c])\n",
    "        \n",
    "        # Attention\n",
    "        attention_out_inference = attention_layer([decoder_output_p, decoder_input_state_hidden])\n",
    "        decoder_concat = Concatenate(axis=-1, name=\"concat_attention\")([decoder_output_p, attention_out_inference])\n",
    "        \n",
    "        # Final prediction\n",
    "        decoder_final_output = decoder_dense(decoder_concat)\n",
    "        \n",
    "        # make the final decoder model\n",
    "        return tf.keras.models.Model([decoder_input, decoder_input_state_hidden, decoder_input_state_h, decoder_input_state_c], [decoder_final_output, state_h_p, state_c_p])\n",
    "\n",
    "    train_model = tf.keras.models.Model([encoder_input, decoder_input], decoder_output)\n",
    "    train_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "    return train_model, standalone_encoder(), standalone_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb235ea-98d7-4869-ac17-3a6ff4a37edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc_lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc_lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc_lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc_lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc_lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc_lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer decoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer decoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_embeding (Embedding)   (None, None, 90)     6570        ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " enc_lstm_1 (LSTM)              [(None, None, 250),  341000      ['encoder_embeding[0][0]']       \n",
      "                                 (None, 250),                                                     \n",
      "                                 (None, 250)]                                                     \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " enc_lstm_2 (LSTM)              [(None, None, 250),  501000      ['enc_lstm_1[0][0]']             \n",
      "                                 (None, 250),                                                     \n",
      "                                 (None, 250)]                                                     \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 90)     17370       ['decoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " enc_lstm_3 (LSTM)              [(None, None, 250),  501000      ['enc_lstm_2[0][0]']             \n",
      "                                 (None, 250),                                                     \n",
      "                                 (None, 250)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 250),  341000      ['embedding[0][0]',              \n",
      "                                 (None, 250),                     'enc_lstm_3[0][1]',             \n",
      "                                 (None, 250)]                     'enc_lstm_3[0][2]']             \n",
      "                                                                                                  \n",
      " attention_layer (Attention)    (None, None, 250)    0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'enc_lstm_3[0][0]']             \n",
      "                                                                                                  \n",
      " decoder_concat (Concatenate)   (None, None, 500)    0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 193)   96693       ['decoder_concat[0][0]']         \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,804,633\n",
      "Trainable params: 1,804,633\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_model, encoder_model, decoder_model = define_model(tokenizer_input.num_words+1, tokenizer_target.num_words+1)\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5449d6e2-f4c4-4c57-ab28-55268b7f608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.load_weights(\"w.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c4994-28c4-40b3-a789-3a13a60cdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=4, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7688b21d-3acd-4281-af16-66fb4b515ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and it's weights\n",
    "model.save_weights(\"w.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7157b907-5628-4e8c-bf45-49090d1d63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_decoder(input_sequence):\n",
    "    \"\"\"\n",
    "    This function decods the sequence predicted\n",
    "    \"\"\"\n",
    "    # encode the input \n",
    "    encoder_output, encoder_h, encoder_c = encoder_model.predict(input_sequence, verbose=0)\n",
    "    # Define an embpy target with it's first woed set to input sequence \n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = tokenizer_target.word_index[\"soseq\"]\n",
    "\n",
    "    # define variables for when to stop and the decoded sentence\n",
    "    decoded_sentence = \"\"\n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        # when we did not reach the place to stop (eoseq), decode \n",
    "        output_token, h, c = decoder_model.predict([target_sequence, encoder_output, encoder_h, encoder_c], verbose=0)\n",
    "\n",
    "        # predict token\n",
    "        predicted_token_index = np.argmax(output_token[0, -1, :])\n",
    "        predicted_token = tokenizer_target.index_word[predicted_token_index]\n",
    "        \n",
    "        # if the token was not end of sequence, continue and add the predicted token to the dcecoded sentence variable \n",
    "        if  predicted_token != \"eoseq\":\n",
    "            decoded_sentence += \" \" + predicted_token\n",
    "\n",
    "        # if we've reached eoseq token or reached the summary length limit, make the stop state true \n",
    "        if predicted_token == \"eoseq\":\n",
    "            stop = True\n",
    "\n",
    "        # update the target sequence \n",
    "        target_sequence = np.zeros((1,1))\n",
    "        target_sequence[0, 0] = predicted_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        encoder_h, encoder_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23de8df0-b334-47a3-9eb0-e9ff509cc118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Review:\n",
      " i just to you know that i a really time with you yeah i really as well if you are up for it i would really like to see you i am but i have a really up yeah no i but if you to go just me know yeah of for sure have a\n",
      "-Original summary:\n",
      " and tonight would like to meet but is\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " what was the of the i will you the i get\n",
      "-Original summary:\n",
      " will a to the when he home\n",
      "-Predicted summary:\n",
      "  and are going to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " can i your sure it will on you is it i it to when is she it to you not sure how she it a of you her and if she it i do not to be you are not it is your all she just to it on i am going on a with this and we are going to the i have to good ok ok i will one you will me your you have a\n",
      "-Original summary:\n",
      " would like to she is going to the with her date the to a of and will have to her to give it back\n",
      "-Predicted summary:\n",
      "  and are a and will meet to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " file the is that go to\n",
      "-Original summary:\n",
      " sent\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " how are d i am not at at i do not have to that i am with my we are for i have just for i think i have a to and how are you d my for the so i am the to when i have some time i on the the yeah you have to the of a he he d from that i with my she was to see d what about you have you i a do not the about how he to about to the it is a really to be i but you did just what i have he he d what is the about is it that is the the is as the about the you like but it on the of the and i have to the was really well and you to think about that is as for the i know a of i do not that some to he he i am not one of\n",
      "-Original summary:\n",
      " has a lot of work he is for a with his and he is parents for the week so she is she also to see with her a book do not\n",
      "-Predicted summary:\n",
      "  and are a and will meet to the the\n",
      "\n",
      "\n",
      "-Review:\n",
      " what is for but we and no\n",
      "-Original summary:\n",
      " and are going to have for dinner but they still to get\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " i have just the from the and just i have no how you do it your is on just how i am it is no really well a me all your i think it is really all about the but there is no that for so it is is the i file me about it this one for my and i it but i am sure you know that so for you but now what i to really it it this file a yeah i think that is what a for some i do not a really one but it me you\n",
      "-Original summary:\n",
      " is by on the from the party recommends the like and\n",
      "-Predicted summary:\n",
      "  and are a and will meet to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " can you and me do my are you there i did not your at so you is and not know but you know it is now the time i am at your it will be do you my now no will do ok i will be there when you from you are the you\n",
      "-Original summary:\n",
      " will help tomorrow\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " can i your sure but what with do not now and i have no time to it i am ok ok and it\n",
      "-Original summary:\n",
      " peter will car\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " file really in it is and file not in i am my for a i am in the oh no on it is not that have a of you you too\n",
      "-Original summary:\n",
      " is in and it is there she is in the for her in place the is\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " good no not really oh yeah a really good one is i it what is it well it is of like in the of you know i really like well the i was about a you know that i did not know he a yeah it is really good and it is really it with the he as a it is a really good do you have at i have but the a is on my ok can you it to sure you know that there is a at yeah but it is all i have the there ok see you\n",
      "-Original summary:\n",
      " asks to some good has by and he can it to from that he is a about a has and a on his he will bring it to work tomorrow\n",
      "-Predicted summary:\n",
      "  and are a and will meet to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " not this what i but a she is all you see will what to do i to get to this when good for so when will to one of on i it is what so how was it what how did do it i just not what did she that and it you will d\n",
      "-Original summary:\n",
      " up with\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " ok well what is i so all the time and it is what your that it will go one the is i am to going to in the it was when i did not know it file when are you i will do that he is good he will know what to do ok he i will go\n",
      "-Original summary:\n",
      " from and she is a she has an with her for\n",
      "-Predicted summary:\n",
      "  and are a for the\n",
      "\n",
      "\n",
      "-Review:\n",
      " what time we not how it to get to about ok so we about would be good i do not to be if there are some so of so i will be at your at\n",
      "-Original summary:\n",
      " will meet and tomorrow at 8\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " i just a from the yeah they the there some when will they it on at but i to be at my ok that is i will be i will the and i will on\n",
      "-Original summary:\n",
      " about in date of the it is on pm has her 5 pm will pick up the on friday he will stay at work\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n",
      "-Review:\n",
      " what time we i not it is a at ok so it we can i really do not ok i too as well sure me i am we in not for of i will i will a for of it\n",
      "-Original summary:\n",
      " has a day at work and will meet after in will the place and book a has to work tomorrow\n",
      "-Predicted summary:\n",
      "  and are to the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 16):\n",
    "    print(\"-Review:\\n\", tokenizer_input.sequences_to_texts([input_sequences[i]])[0])\n",
    "    print(\"-Original summary:\\n\", tokenizer_target.sequences_to_texts([target_sequences[i]])[0][6:-6])\n",
    "    print(\"-Predicted summary:\\n\", sequence_decoder(input_sequences[i].reshape(1,-1)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f6ed5-71df-400d-9a4e-9d3a5d2e0c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
